# -*- coding: utf-8 -*-
"""voiceIntegration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s26zobXqsAqRqiGVkCNHJlf4OdQgvyE1
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/V1zai/Vizai.git
# %cd Vizai

!pip install playsound

import cv2
from ultralytics import YOLO
from gtts import gTTS
from playsound import playsound # For playing audio
import threading
import os
import time

from ultralytics import YOLO
from gtts import gTTS
from IPython.display import Audio, display

# --- Global Variables for Thread Communication ---
target_object_label = None
announce_on_find = False
last_command = None
announced_objects_in_frame = set() # To prevent spamming TTS
stop_threads = False # Flag to signal threads to stop

model = YOLO("yolov8n.pt")

def speak_text(text_to_speak, lang='en'):
    try:
        print(f"TTS: Attempting to say: '{text_to_speak}'")
        tts = gTTS(text=text_to_speak, lang=lang, slow=False)
        # Using a unique name to avoid conflicts if TTS is rapid, though playsound is blocking
        audio_file = f"temp_tts_audio_{int(time.time()*1000)}.mp3"
        tts.save(audio_file)
        playsound(audio_file)
        os.remove(audio_file) # Clean up the audio file
        # print("TTS: Playback complete.")
    except Exception as e:
        print(f"Error in TTS or playback: {e}")
        if "audio_file" in locals() and os.path.exists(audio_file):
            try:
                os.remove(audio_file) # Attempt cleanup on error too
            except Exception as e_del:
                print(f"Error deleting temp audio file: {e_del}")

def main_realtime_detection():
    global target_object_label, announce_on_find, stop_threads, announced_objects_in_frame, yolo_model

    cap = cv2.VideoCapture(0) # 0 for default camera, or path to video file
    if not cap.isOpened():
        print("Error: Could not open video stream (webcam or file).")
        stop_threads = True # Signal voice thread to stop
        return

    print("\n--- Real-time Object Detection Started ---")
    print("Press 'q' in the video window to quit.")
    # You can optionally initialize a new ClearML task here for logging this inference session
    # from clearml import Task
    # inference_task = Task.init(project_name="Vizai/Inference", task_name="Realtime Detection with Voice")

    frame_count = 0
    while not stop_threads:
        ret, frame = cap.read()
        if not ret:
            print("Error: Can't receive frame (stream end?). Exiting ...")
            break

        frame_count += 1
        # Process every Nth frame if performance is an issue
        # if frame_count % 2 != 0:
        #     cv2.imshow('Real-time Object Detection', frame)
        #     if cv2.waitKey(1) & 0xFF == ord('q'):
        #         stop_threads = True
        #     continue


        # Perform YOLO detection on the frame
        results = yolo_model(frame, verbose=False) # verbose=False to reduce console spam from YOLO

        objects_to_announce_this_frame = []

        if results and results[0].boxes.cls.numel() > 0:
            detected_boxes = results[0].boxes
            for i in range(len(detected_boxes.cls)):
                cls_id = int(detected_boxes.cls[i])
                # Use model.names which should be populated by YOLOv8 based on the loaded model
                # Default to 'unknown' if class_id is out of bounds for some reason
                label = yolo_model.names.get(cls_id, f"class_{cls_id}") if hasattr(yolo_model, 'names') and isinstance(yolo_model.names, dict) else f"class_{cls_id}"

                confidence = float(detected_boxes.conf[i])
                bbox = detected_boxes.xyxy[i].cpu().numpy().astype(int) # [x1, y1, x2, y2]
                x1, y1, x2, y2 = bbox

                display_label_text = f"{label} ({confidence:.2f})"
                color = (255, 0, 0) # Default color: Blue for non-target objects

                # If a specific object is targeted by voice command
                if target_object_label and label.lower() == target_object_label.lower():
                    color = (0, 255, 0) # Highlight color: Green for target
                    display_label_text = f"TARGET: {label} ({confidence:.2f})"

                    # Announce if it's the first time finding this specific target object in a frame
                    # since the command was given for this target.
                    if announce_on_find and label not in announced_objects_in_frame:
                        objects_to_announce_this_frame.append(f"{label} found")
                        announced_objects_in_frame.add(label) # Mark as "session announced" for this target
                        # Set announce_on_find to False after the first group of target objects are announced.
                        # This prevents re-announcing if multiple instances of the same target are found sequentially.
                        # announce_on_find = False # Turn off general announce flag once any target is found and queued for announcement

                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                cv2.putText(frame, display_label_text, (x1, y1 - 10),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        else:
            # No objects detected in this frame
            pass

        # After processing all detections in the frame, if there were target objects found and
        # announce_on_find was true, make announce_on_find false.
        # This ensures we only announce the first frame(s) where the target is found after a command.
        if objects_to_announce_this_frame and any(target_object_label and obj.startswith(target_object_label) for obj in objects_to_announce_this_frame):
             announce_on_find = False


        # Handle announcements for this frame (TTS runs in its own thread)
        if objects_to_announce_this_frame:
            announcement_text = ", ".join(objects_to_announce_this_frame)
            # Run TTS in a separate thread to avoid blocking video
            tts_thread = threading.Thread(target=speak_text, args=(announcement_text,))
            tts_thread.daemon = True # Allow main program to exit even if TTS thread is active
            tts_thread.start()


        cv2.imshow('Real-time Object Detection with Voice Command', frame)

        # Check for 'q' key press to quit
        key = cv2.waitKey(1) & 0xFF
        if key == ord('q'):
            print("Quit command (q key) received from video window.")
            stop_threads = True # Signal threads to stop
            break

        if stop_threads: # Check flag if voice command initiated exit
            break

    # Release resources
    cap.release()
    cv2.destroyAllWindows()
    print("Video stream and windows closed.")
    stop_threads = True # Ensure voice thread also knows to stop

if __name__ == "__main__":
    print("Starting Real-Time Object Detection with Voice Commands...")
    print(f"Attempting to use YOLO model from: {model}")
    print("Please ensure you have a microphone connected for voice commands.")
    print("Make sure you have installed all required libraries:")
    print("  pip install opencv-python ultralytics gTTS playsound SpeechRecognition PyAudio")
    print("(PyAudio might require special installation steps depending on your OS)")
    print("Optional for ClearML logging: pip install clearml")
    print("-" * 30)



